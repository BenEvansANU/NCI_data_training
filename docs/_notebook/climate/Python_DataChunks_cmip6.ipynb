{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![logo](../../_static/images/NCI_logo.png)\n",
    "\n",
    "\n",
    "# Data Chunking with Dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Launch the Jupyter Notebook application\n",
    "\n",
    "#### Using the public hh5 conda environment managed by CLEX\n",
    "\n",
    "Many python modules are available under the hh5 conda environment that is maintained by CLEX, as well as additional modules such as that of CleF used in the previous examples. This environment is publically available and developed to service the CLEX users allowing use cases for the wider communinty.\n",
    "```\n",
    "    $ module use /g/data3/hh5/public/modules\n",
    "    $ module load conda/analysis3-unstable\n",
    "```  \n",
    "\n",
    "Launch the Jupyter Notebook application:\n",
    "```\n",
    "    $ jupyter notebook\n",
    "``` \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>NOTE: </b> This will launch the <b>Notebook Dashboard</b> within a new web browser window. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening Multiple Files at Once\n",
    "\n",
    "xarray's `open_mfdataset` allows multiple files to be opened simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pr_day_GFDL-CM4_ssp585_r1i1p1f1_gr1_20150101-20341231.nc\r\n",
      "pr_day_GFDL-CM4_ssp585_r1i1p1f1_gr1_20350101-20541231.nc\r\n",
      "pr_day_GFDL-CM4_ssp585_r1i1p1f1_gr1_20550101-20741231.nc\r\n",
      "pr_day_GFDL-CM4_ssp585_r1i1p1f1_gr1_20750101-20941231.nc\r\n",
      "pr_day_GFDL-CM4_ssp585_r1i1p1f1_gr1_20950101-21001231.nc\r\n"
     ]
    }
   ],
   "source": [
    "!ls /g/data/oi10/replicas/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-CM4/ssp585/r1i1p1f1/day/pr/gr1/v20180701\n",
    "path = '/g/data/oi10/replicas/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-CM4/ssp585/r1i1p1f1/day/pr/gr1/v20180701/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In a similar way to open_dataset, use open_mfdataset to open the multiple files in `path` and have a look at the Dataset. Do you see anything different from the previous notebook?\n",
    "\n",
    "<a href=\"#ans1\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans1\">\n",
    "<pre><code>\n",
    "f_ssp585 = xr.open_mfdataset(path)\n",
    "f_ssp585\n",
    "</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks\n",
    "\n",
    "Notice that it says:\n",
    "`pr(time, lat, lon) float32 dask.array<shape=(31390, 180, 288), chunksize=(7300, 180, 288)`\n",
    "There is now an additional component to the shape, and that is `chunksize`.\n",
    "\n",
    "The chunking of the array comes from the integration of Dask with xarray. Dask (see: https://docs.dask.org/en/latest/) is a library for parallel computing. Dask divides the data array into small pieces called \"chunks\", with each chunk designed to be small enough to fit into memory. \n",
    "\n",
    "The file itself may be already chunked. Filesystem chunking is available in netCDF-4 and HDF5 datasets. CMIP6 data should all be netCDF-4 and include some form of chunking on the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netcdf pr_day_GFDL-CM4_ssp585_r1i1p1f1_gr1_20150101-20341231 {\r\n",
      "dimensions:\r\n",
      "\tlat = 180 ;\r\n",
      "\tbnds = 2 ;\r\n",
      "\tlon = 288 ;\r\n",
      "\ttime = UNLIMITED ; // (7300 currently)\r\n",
      "variables:\r\n",
      "\tdouble lat(lat) ;\r\n",
      "\t\tlat:long_name = \"latitude\" ;\r\n",
      "\t\tlat:units = \"degrees_north\" ;\r\n",
      "\t\tlat:axis = \"Y\" ;\r\n",
      "\t\tlat:bounds = \"lat_bnds\" ;\r\n",
      "\t\tlat:standard_name = \"latitude\" ;\r\n",
      "\t\tlat:cell_methods = \"time: point\" ;\r\n",
      "\t\tlat:_Storage = \"chunked\" ;\r\n",
      "\t\tlat:_ChunkSizes = 180 ;\r\n",
      "\t\tlat:_DeflateLevel = 2 ;\r\n",
      "\t\tlat:_Shuffle = \"true\" ;\r\n",
      "\t\tlat:_Endianness = \"little\" ;\r\n",
      "\tdouble lat_bnds(lat, bnds) ;\r\n",
      "\t\tlat_bnds:long_name = \"latitude bounds\" ;\r\n",
      "\t\tlat_bnds:units = \"degrees_north\" ;\r\n",
      "\t\tlat_bnds:axis = \"Y\" ;\r\n",
      "\t\tlat_bnds:_Storage = \"chunked\" ;\r\n",
      "\t\tlat_bnds:_ChunkSizes = 180, 2 ;\r\n",
      "\t\tlat_bnds:_DeflateLevel = 2 ;\r\n",
      "\t\tlat_bnds:_Shuffle = \"true\" ;\r\n",
      "\t\tlat_bnds:_Endianness = \"little\" ;\r\n",
      "\tdouble lon(lon) ;\r\n",
      "\t\tlon:long_name = \"longitude\" ;\r\n",
      "\t\tlon:units = \"degrees_east\" ;\r\n",
      "\t\tlon:axis = \"X\" ;\r\n",
      "\t\tlon:bounds = \"lon_bnds\" ;\r\n",
      "\t\tlon:standard_name = \"longitude\" ;\r\n",
      "\t\tlon:cell_methods = \"time: point\" ;\r\n",
      "\t\tlon:_Storage = \"chunked\" ;\r\n",
      "\t\tlon:_ChunkSizes = 288 ;\r\n",
      "\t\tlon:_DeflateLevel = 2 ;\r\n",
      "\t\tlon:_Shuffle = \"true\" ;\r\n",
      "\t\tlon:_Endianness = \"little\" ;\r\n",
      "\tdouble lon_bnds(lon, bnds) ;\r\n",
      "\t\tlon_bnds:long_name = \"longitude bounds\" ;\r\n",
      "\t\tlon_bnds:units = \"degrees_east\" ;\r\n",
      "\t\tlon_bnds:axis = \"X\" ;\r\n",
      "\t\tlon_bnds:_Storage = \"chunked\" ;\r\n",
      "\t\tlon_bnds:_ChunkSizes = 288, 2 ;\r\n",
      "\t\tlon_bnds:_DeflateLevel = 2 ;\r\n",
      "\t\tlon_bnds:_Shuffle = \"true\" ;\r\n",
      "\t\tlon_bnds:_Endianness = \"little\" ;\r\n",
      "\tfloat pr(time, lat, lon) ;\r\n",
      "\t\tpr:long_name = \"Precipitation\" ;\r\n",
      "\t\tpr:units = \"kg m-2 s-1\" ;\r\n",
      "\t\tpr:missing_value = 1.e+20f ;\r\n",
      "\t\tpr:_FillValue = 1.e+20f ;\r\n",
      "\t\tpr:cell_methods = \"area: time: mean\" ;\r\n",
      "\t\tpr:cell_measures = \"area: areacella\" ;\r\n",
      "\t\tpr:standard_name = \"precipitation_flux\" ;\r\n",
      "\t\tpr:interp_method = \"conserve_order1\" ;\r\n",
      "\t\tpr:original_name = \"pr\" ;\r\n",
      "\t\tpr:_Storage = \"chunked\" ;\r\n",
      "\t\tpr:_ChunkSizes = 1, 180, 288 ;\r\n",
      "\t\tpr:_DeflateLevel = 2 ;\r\n",
      "\t\tpr:_Shuffle = \"true\" ;\r\n",
      "\t\tpr:_Endianness = \"little\" ;\r\n",
      "\tdouble time(time) ;\r\n",
      "\t\ttime:long_name = \"time\" ;\r\n",
      "\t\ttime:units = \"days since 1850-01-01 00:00:00\" ;\r\n",
      "\t\ttime:axis = \"T\" ;\r\n",
      "\t\ttime:calendar_type = \"noleap\" ;\r\n",
      "\t\ttime:calendar = \"noleap\" ;\r\n",
      "\t\ttime:bounds = \"time_bnds\" ;\r\n",
      "\t\ttime:standard_name = \"time\" ;\r\n",
      "\t\ttime:description = \"Temporal mean\" ;\r\n",
      "\t\ttime:_Storage = \"chunked\" ;\r\n",
      "\t\ttime:_ChunkSizes = 1 ;\r\n",
      "\t\ttime:_DeflateLevel = 2 ;\r\n",
      "\t\ttime:_Shuffle = \"true\" ;\r\n",
      "\t\ttime:_Endianness = \"little\" ;\r\n",
      "\tdouble time_bnds(time, bnds) ;\r\n",
      "\t\ttime_bnds:long_name = \"time axis boundaries\" ;\r\n",
      "\t\ttime_bnds:units = \"days since 1850-01-01 00:00:00\" ;\r\n",
      "\t\ttime_bnds:_Storage = \"chunked\" ;\r\n",
      "\t\ttime_bnds:_ChunkSizes = 1, 2 ;\r\n",
      "\t\ttime_bnds:_DeflateLevel = 2 ;\r\n",
      "\t\ttime_bnds:_Shuffle = \"true\" ;\r\n",
      "\t\ttime_bnds:_Endianness = \"little\" ;\r\n",
      "\r\n",
      "// global attributes:\r\n",
      "\t\t:external_variables = \"areacella\" ;\r\n",
      "\t\t:history = \"File was processed by fremetar (GFDL analog of CMOR). TripleID: [exper_id_8FFywjr5HC,realiz_id_Lda2LtjE2s,run_id_Dj7oSa1Fj7]\" ;\r\n",
      "\t\t:table_id = \"day\" ;\r\n",
      "\t\t:activity_id = \"ScenarioMIP\" ;\r\n",
      "\t\t:branch_method = \"standard\" ;\r\n",
      "\t\t:branch_time_in_child = 0. ;\r\n",
      "\t\t:comment = \"<null ref>\" ;\r\n",
      "\t\t:contact = \"gfdl.climate.model.info@noaa.gov\" ;\r\n",
      "\t\t:Conventions = \"CF-1.7 CMIP-6.0 UGRID-1.0\" ;\r\n",
      "\t\t:creation_date = \"2019-03-18T15:13:56Z\" ;\r\n",
      "\t\t:data_specs_version = \"01.00.27\" ;\r\n",
      "\t\t:experiment = \"update of RCP8.5 based on SSP5\" ;\r\n",
      "\t\t:experiment_id = \"ssp585\" ;\r\n",
      "\t\t:forcing_index = 1 ;\r\n",
      "\t\t:frequency = \"day\" ;\r\n",
      "\t\t:further_info_url = \"https://furtherinfo.es-doc.org/CMIP6.NOAA-GFDL.GFDL-CM4.ssp585.none.r1i1p1f1\" ;\r\n",
      "\t\t:grid = \"atmos data regridded from Cubed-sphere (c96) to 180,288; interpolation method: conserve_order1\" ;\r\n",
      "\t\t:grid_label = \"gr1\" ;\r\n",
      "\t\t:initialization_index = 1 ;\r\n",
      "\t\t:institution = \"National Oceanic and Atmospheric Administration, Geophysical Fluid Dynamics Laboratory, Princeton, NJ 08540, USA\" ;\r\n",
      "\t\t:institution_id = \"NOAA-GFDL\" ;\r\n",
      "\t\t:license = \"CMIP6 model data produced by NOAA-GFDL is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License (https://creativecommons.org/licenses/). Consult https://pcmdi.llnl.gov/CMIP6/TermsOfUse for terms of use governing CMIP6 output, including citation requirements and proper acknowledgment. Further information about this data, including some limitations, can be found via the further_info_url (recorded as a global attribute in this file). The data producers and data providers make no warranty, either express or implied, including, but not limited to, warranties of merchantability and fitness for a particular purpose. All liabilities arising from the supply of the information (including any liability arising in negligence) are excluded to the fullest extent permitted by law.\" ;\r\n",
      "\t\t:mip_era = \"CMIP6\" ;\r\n",
      "\t\t:nominal_resolution = \"100 km\" ;\r\n",
      "\t\t:parent_activity_id = \"CMIP\" ;\r\n",
      "\t\t:parent_experiment_id = \"historical\" ;\r\n",
      "\t\t:parent_mip_era = \"CMIP6\" ;\r\n",
      "\t\t:parent_source_id = \"GFDL-CM4\" ;\r\n",
      "\t\t:parent_variant_label = \"r1i1p1f1\" ;\r\n",
      "\t\t:physics_index = 1 ;\r\n",
      "\t\t:product = \"model-output\" ;\r\n",
      "\t\t:realization_index = 1 ;\r\n",
      "\t\t:realm = \"atmos\" ;\r\n",
      "\t\t:source = \"GFDL-CM4 (2018): \\n\",\r\n",
      "\t\t\t\"aerosol: interactive\\n\",\r\n",
      "\t\t\t\"atmos: GFDL-AM4.0.1 (Cubed-sphere (c96) - 1 degree nominal horizontal resolution; 360 x 180 longitude/latitude; 33 levels; top level 1 hPa)\\n\",\r\n",
      "\t\t\t\"atmosChem: fast chemistry, aerosol only\\n\",\r\n",
      "\t\t\t\"land: GFDL-LM4.0.1 (1 degree nominal horizontal resolution; 360 x 180 longitude/latitude; 20 levels; bot level 10m); land:Veg:unnamed (dynamic vegetation, dynamic land use); land:Hydro:unnamed (soil water and ice, multi-layer snow, rivers and lakes)\\n\",\r\n",
      "\t\t\t\"landIce: GFDL-LM4.0.1\\n\",\r\n",
      "\t\t\t\"ocean: GFDL-OM4p25 (GFDL-MOM6, tripolar - nominal 0.25 deg; 1440 x 1080 longitude/latitude; 75 levels; top grid cell 0-2 m)\\n\",\r\n",
      "\t\t\t\"ocnBgchem: GFDL-BLINGv2\\n\",\r\n",
      "\t\t\t\"seaIce: GFDL-SIM4p25 (GFDL-SIS2.0, tripolar - nominal 0.25 deg; 1440 x 1080 longitude/latitude; 5 layers; 5 thickness categories)\\n\",\r\n",
      "\t\t\t\"(GFDL ID: 2019_0186)\" ;\r\n",
      "\t\t:source_id = \"GFDL-CM4\" ;\r\n",
      "\t\t:source_type = \"AOGCM\" ;\r\n",
      "\t\t:sub_experiment = \"none\" ;\r\n",
      "\t\t:sub_experiment_id = \"none\" ;\r\n",
      "\t\t:title = \"NOAA GFDL GFDL-CM4 model output prepared for CMIP6 update of RCP8.5 based on SSP5\" ;\r\n",
      "\t\t:tracking_id = \"hdl:21.14100/ad8c930c-124f-4b82-98b2-98cc4236ba25\" ;\r\n",
      "\t\t:variable_id = \"pr\" ;\r\n",
      "\t\t:variant_info = \"N/A\" ;\r\n",
      "\t\t:references = \"see further_info_url attribute\" ;\r\n",
      "\t\t:variant_label = \"r1i1p1f1\" ;\r\n",
      "\t\t:branch_time_in_parent = 60225. ;\r\n",
      "\t\t:parent_time_units = \"days since 1850-1-1\" ;\r\n",
      "\t\t:_NCProperties = \"version=2,netcdf=4.6.2,hdf5=1.10.4\" ;\r\n",
      "\t\t:_SuperblockVersion = 2 ;\r\n",
      "\t\t:_IsNetcdf4 = 1 ;\r\n",
      "\t\t:_Format = \"netCDF-4 classic model\" ;\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!ncdump -hst '/g/data/oi10/replicas/CMIP6/ScenarioMIP/NOAA-GFDL/GFDL-CM4/ssp585/r1i1p1f1/day/pr/gr1/v20180701/pr_day_GFDL-CM4_ssp585_r1i1p1f1_gr1_20150101-20341231.nc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this case the file is chunked such that  `pr:_ChunkSizes = 1, 180, 288 ;`\n",
    "\n",
    "Here we see that the data is chunked in time, where one chunk is one time-step and all points in lat-lon.\n",
    "\n",
    "![](images/Chunks.png)\n",
    "image source: https://www.unidata.ucar.edu/blogs/developer/en/entry/chunking_data_why_it_matters\n",
    "\n",
    "Consider 2 types of data access\n",
    "1. Accessing a 2D lat-lon slice in time (RHS figure)\n",
    "2. Accessing a time series at a single lat-lon point (LHS figure)\n",
    "\n",
    "With the chunking above, the first type of data access only needs to access a single chunk, while the second type needs to access ALL the chunks of the data array regardless. This dataset will be fastest for 2D lat-lon single time-step data access.\n",
    "\n",
    "In general, even without chunking - when the data is accessed contiguously (by index order) - time is the slowest variable to access, then y, with x being the fastest. With the chunking method of this CMIP6 dataset, time still remains the slowest variable. For more uniform variable access speeds more evenly spaced chunks would be needed, spacing the chunks in time, lat, and lon.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Time how long it takes to load the precipitation data at `time='2015-01-01'` and then time how long it takes to load the data at `lat=0` and `lon=180` (remember to use `method='nearest'` for the latter case). How much difference is there in the different access methods?\n",
    "\n",
    "<a href=\"#ans4\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans4\">\n",
    "<pre><code>\n",
    "%%time\n",
    "f_ssp585.pr.sel(time='2015-01-01').load()\n",
    "\n",
    "-----------------------------------------------\n",
    "\n",
    "%%time\n",
    "f_ssp585.pr.sel(lat=0,lon=180,method='nearest').load()\n",
    "</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximately the same amount of data took 100x longer to load.\n",
    "\n",
    "The spatial dataset contains 51840 data-points and took order 100ms to load. The time-series dataset has 31390 data-points and took order 10,000 ms to load.\n",
    "\n",
    "Chunking and the ways in which the data is read is important in considering both how you access the data and if you wish to parallelise your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF file Chunks versus Dask Chunks\n",
    "\n",
    "Keep in mind, dask chunking is different to chunking of the stored data. As we saw, the stored data is chunked with chunks of size (1,180,288). The Dask array was chunked with size (7300, 180, 288). You can change the chunking in the dask array. In the below example we are specifying that there be 730 chunks in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (bnds: 2, lat: 180, lon: 288, time: 31390)\n",
       "Coordinates:\n",
       "  * lat        (lat) float64 -89.5 -88.5 -87.5 -86.5 ... 86.5 87.5 88.5 89.5\n",
       "  * lon        (lon) float64 0.625 1.875 3.125 4.375 ... 355.6 356.9 358.1 359.4\n",
       "  * time       (time) object 2015-01-01 12:00:00 ... 2100-12-31 12:00:00\n",
       "Dimensions without coordinates: bnds\n",
       "Data variables:\n",
       "    lat_bnds   (time, lat, bnds) float64 dask.array<shape=(31390, 180, 2), chunksize=(7300, 180, 2)>\n",
       "    lon_bnds   (time, lon, bnds) float64 dask.array<shape=(31390, 288, 2), chunksize=(7300, 288, 2)>\n",
       "    pr         (time, lat, lon) float32 dask.array<shape=(31390, 180, 288), chunksize=(730, 180, 288)>\n",
       "    time_bnds  (time, bnds) object dask.array<shape=(31390, 2), chunksize=(730, 2)>\n",
       "Attributes:\n",
       "    external_variables:     areacella\n",
       "    history:                File was processed by fremetar (GFDL analog of CM...\n",
       "    table_id:               day\n",
       "    activity_id:            ScenarioMIP\n",
       "    branch_method:          standard\n",
       "    branch_time_in_child:   0.0\n",
       "    comment:                <null ref>\n",
       "    contact:                gfdl.climate.model.info@noaa.gov\n",
       "    Conventions:            CF-1.7 CMIP-6.0 UGRID-1.0\n",
       "    creation_date:          2019-03-18T15:13:56Z\n",
       "    data_specs_version:     01.00.27\n",
       "    experiment:             update of RCP8.5 based on SSP5\n",
       "    experiment_id:          ssp585\n",
       "    forcing_index:          1\n",
       "    frequency:              day\n",
       "    further_info_url:       https://furtherinfo.es-doc.org/CMIP6.NOAA-GFDL.GF...\n",
       "    grid:                   atmos data regridded from Cubed-sphere (c96) to 1...\n",
       "    grid_label:             gr1\n",
       "    initialization_index:   1\n",
       "    institution:            National Oceanic and Atmospheric Administration, ...\n",
       "    institution_id:         NOAA-GFDL\n",
       "    license:                CMIP6 model data produced by NOAA-GFDL is license...\n",
       "    mip_era:                CMIP6\n",
       "    nominal_resolution:     100 km\n",
       "    parent_activity_id:     CMIP\n",
       "    parent_experiment_id:   historical\n",
       "    parent_mip_era:         CMIP6\n",
       "    parent_source_id:       GFDL-CM4\n",
       "    parent_variant_label:   r1i1p1f1\n",
       "    physics_index:          1\n",
       "    product:                model-output\n",
       "    realization_index:      1\n",
       "    realm:                  atmos\n",
       "    source:                 GFDL-CM4 (2018): \\naerosol: interactive\\natmos: G...\n",
       "    source_id:              GFDL-CM4\n",
       "    source_type:            AOGCM\n",
       "    sub_experiment:         none\n",
       "    sub_experiment_id:      none\n",
       "    title:                  NOAA GFDL GFDL-CM4 model output prepared for CMIP...\n",
       "    tracking_id:            hdl:21.14100/ad8c930c-124f-4b82-98b2-98cc4236ba25\n",
       "    variable_id:            pr\n",
       "    variant_info:           N/A\n",
       "    references:             see further_info_url attribute\n",
       "    variant_label:          r1i1p1f1\n",
       "    branch_time_in_parent:  60225.0\n",
       "    parent_time_units:      days since 1850-1-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_ssp585 = xr.open_mfdataset(path,chunks={'time':730})\n",
    "\n",
    "f_ssp585"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How big do you make your chunks?\n",
    "\n",
    "The rule of thumb for dask chunks is that you should \"create arrays with a minimum chunksize of at least one million elements\":  http://xarray.pydata.org/en/stable/dask.html#chunking-and-performance\n",
    "\n",
    "netCDF4 chunks are often a lot smaller than dask array chunks. The minimum chunksize exists because if you have too many chunks, then queuing of operations when parallising will be slow, and if they are too big computation and memory can be wasted. The default chunks from dask gave us chunks of size: (7300, 180, 288) or nearly 400 million elements so we could try reducing those chunks if needed. The larger the array, the larger the cost of queueing and the larger chunks may be needed.\n",
    "\n",
    "#### IMPORTANT: Whatever dask array chunks you use, make sure they align with the netCDF4 file chunks!!\n",
    "\n",
    "So far our chunks have been in time, and the netCDF4 file is also chunked in time. If we tried to use dask chunks to optimise the time-series loading of data, it will not help! \n",
    "\n",
    "#### Exercise\n",
    "\n",
    "Try it, load the data with chunks size `(31390,180,1)` (i.e. chunked in lon) and name that file `f_bad_chunk`. Try re-loading the time series of pr at `lat=0` and `lon=180` and time how long it takes.\n",
    "\n",
    "<a href=\"#ans1\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans1\">\n",
    "<pre><code>\n",
    "f_bad_chunk = xr.open_mfdataset(path,chunks={'time':31390,'lat':180,'lon':1})\n",
    "----------------------------------------\n",
    "%%time\n",
    "f_bad_chunk.pr.sel(lat=0,lon=180,method='nearest').load()\n",
    "</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we add some extra cores to the computation?\n",
    "\n",
    "You can easily parallelise xarray code using the dask.distributed.Client and dask array calculations will be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "c = Client()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Try running your previous code for `f_bad_chunk` again loading the time series of pr at `lat=0` and `lon=180` and time how long it takes now that there are 4 workers in the dask cluster.\n",
    "\n",
    "Do the same with the original chunking method of `f_ssp585` and see if there is a difference.\n",
    "\n",
    "<a href=\"#ans2\" data-toggle=\"collapse\">Answer</a>\n",
    "<div class=\"collapse\" id=\"ans2\">\n",
    "<pre><code>\n",
    "%%time\n",
    "f_bad_chunk.pr.sel(lat=0,lon=180,method='nearest').load()\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "%%time\n",
    "f_ssp585.pr.sel(lat=0,lon=180,method='nearest').load()\n",
    "</code></pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poor chunking with dask can make your performance worse!\n",
    "\n",
    "Both the size of the chunks and the alignment of the chunks with the filesystem chunks are imporant to keep in mind when creating dask chunks. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
